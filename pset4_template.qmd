---
title: "Problem Set 4"
author: "Ben Schiffman and Hallie Lovin"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

```{python}
#import necessary packages
import pandas as pd 
import altair as alt
```

1. 
The variables that I pulled were: PRVDR_CTGRY_SBTYP_CD - shows the subtype of the provider
PRVDR_CTGRY_CD - shows the type of provider 
CITY_NAME - shows the city that the provider is physically located in 
FAC_NAME - shows the name of the provider that is providing services 
PRVDR_NUM - shows the CMS certification number 
STATE_CD - shows the state abbreviation
ST_ADR - shows the street address where the provider is located 
PGM_TRMNTN_CD - shows the termination status of the provider 
TRMNTN_EXPRTN_DT - shows the date that the provider was terminated 
ZIP_CD - shows the zip code of the providers physical address 
FIPS_STATE_CD - shows the FIPS state code 
CBSA_URBN_RRL_IND - shows if the town is urban or rural
2. 

```{python}
#import the data 
import os

path = r"/Users/hallielovin/Documents/GitHub/problem-set-4-ben_hallie"
ben_path = r"/Users/benschiffman/Desktop/Python 2/problem-set-4-ben_hallie"
pos2016 = r"pos2016.csv"

pos2016_df = pd.read_csv(os.path.join(path, pos2016))

#convert zip code to a string 
pos2016_df["ZIP_CD"] = pos2016_df["ZIP_CD"].astype(str).str.replace(".0", " ", regex = False)

pos2016_df.head()
```

```{python}
#subset data to only include those with provider type code 1 and subtype code 1
pos2016_df = pos2016_df[(pos2016_df["PRVDR_CTGRY_CD"] == 1) & (pos2016_df["PRVDR_CTGRY_SBTYP_CD"] == 1)]
```

```{python}
#determine how many hospitals are reported in the data 
len(pos2016_df)
```

    a. There are 7,245 hospitals reported in the data that are considered "short term" hospitals. According to the brief by the Kaiser Family Foundation, there are around 5,000 short term, acute care hospitals in the US. That said, the numbers in this data seem much higher. 

    b. After looking at more sources, a CMS report showed that there were 3,436 medicaid participating hospitals in 2016. The reasons that all of these numbers may differ is because some hospitals may have closed which we have not yet accounted for in our data. Additionally, the Kaiser Foundation is looking at all hospitals that are short term and not filtering out those that bill medicare and medicaid. 
3. 

```{python}
#load in the data from 2017, 2018, and 2019

##2017
pos2017 = r"pos2017.csv"

pos2017_df = pd.read_csv(os.path.join(path, pos2017))

#convert zips to string 
pos2017_df["ZIP_CD"] = pos2017_df["ZIP_CD"].astype(str).str.replace(".0", " ", regex = False)

pos2017_df.head()
```

```{python}
##2018-- I was getting an error (UnicodeDecodeError) so I had to get chatgpt to help 
pos2018 = r"pos2018.csv"

pos2018_df = pd.read_csv(os.path.join(path, pos2018), encoding="ISO-8859-1")

#convert zip to string 
pos2018_df["ZIP_CD"] = pos2018_df["ZIP_CD"].astype(str).str.replace(".0", " ", regex = False)

pos2018_df.head()
```
```{python}
##2019-- I was getting an error (UnicodeDecodeError) so I had to get chatgpt to help 
pos2019 = r"pos2019.csv"

pos2019_df = pd.read_csv(os.path.join(path, pos2019), encoding="ISO-8859-1")

#convert zip to string 
pos2019_df["ZIP_CD"] = pos2019_df["ZIP_CD"].astype(str).str.replace(".0", " ", regex = False)

pos2019_df.head()
```

```{python}
#subset out the dataframes so only the code 1's are shown 
##2017 
pos2017_df = pos2017_df[(pos2017_df["PRVDR_CTGRY_CD"] == 1) & (pos2017_df["PRVDR_CTGRY_SBTYP_CD"] == 1)]
```

```{python}
##2018
pos2018_df = pos2018_df[(pos2018_df["PRVDR_CTGRY_CD"] == 1) & (pos2018_df["PRVDR_CTGRY_SBTYP_CD"] == 1)]
```

```{python}
##2019
pos2019_df = pos2019_df[(pos2019_df["PRVDR_CTGRY_CD"] == 1) & (pos2019_df["PRVDR_CTGRY_SBTYP_CD"] == 1)]
```
```{python}
#append all of the data sets together into one pos data set using the CMS certification number (prvdr_num)

##add a year column to each dataset 
pos2016_df["YEAR"] = 2016
pos2017_df["YEAR"] = 2017
pos2018_df["YEAR"] = 2018
pos2019_df["YEAR"] = 2019
```

```{python}
##make the large dataframe
pos_df = pd.concat([pos2016_df, pos2017_df, pos2018_df, pos2019_df], ignore_index = True)
```

```{python}
#group the dataset by year and take the count of how many observations are in each year 
pos_years_count = pos_df.groupby("YEAR").agg(
  NUM_OBSERVATIONS = ("YEAR", "size")
).reset_index()
```

```{python}
#make a plot of the number of observations by year 
alt.Chart(pos_years_count).mark_bar().encode(
  alt.X("YEAR:O"), 
  alt.Y("NUM_OBSERVATIONS:Q")
)
```

4. 
    a. 
```{python}
#make a new dataframe that groups the data by the year counting the unique provider numbers
pos_unique_count = pos_df.groupby("YEAR")["PRVDR_NUM"].nunique().reset_index()
```

```{python}
#make a chart 
alt.Chart(pos_unique_count).mark_bar().encode(
  alt.X("YEAR:O"), 
  alt.Y("PRVDR_NUM:Q")
)
```

    b. These plots look the exact same and that is because the numbers are the same that it is taking in. This tells us that the data is only contains one observation for each hospital in each year. There are no duplicates for the hospitals.

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
#making 2016 list of actives
active_2016 = pos_df[pos_df["PGM_TRMNTN_CD"] == 0]
active_2016 = active_2016[active_2016["YEAR"] == 2016]
len(active_2016)

#sort by 2016 actives
term_p2016 = pos_df[(pos_df["PRVDR_NUM"].isin(active_2016["PRVDR_NUM"])) & (pos_df["YEAR"]> 2016)]

#finding rows where termination code isnt 0
term_p2016 = term_p2016[term_p2016["PGM_TRMNTN_CD"] != 0]

#group by facility name then only retain the lowest year
term_p2016 = term_p2016.groupby("PRVDR_NUM")[["FAC_NAME", "ZIP_CD", "YEAR"]].min().reset_index()
term_p2016.head()

print(len(term_p2016), "hospitals fit this definition")
#consult with hallie on repeats by different zips
```

2. 
```{python}
#sort values by facility name
term_p2016 = term_p2016.sort_values(by = "FAC_NAME")

#printing top 10 results
print(term_p2016.head(10))
```
3. 
    a.
```{python}
#first create a df that will count active hospitals by zip by year
active_zips = pos_df.copy()
active_zips["terminated"] = active_zips["PGM_TRMNTN_CD"].apply(lambda x: "no" if x == 0 else "yes")
active_zips["ZIP_CD"]

zips_term_counts = active_zips.groupby(["ZIP_CD", "terminated", "YEAR"]).size().reset_index(name = "count")

#test that there are actually unique results in here
zips_term_counts["count"].unique()
zips_term_counts = zips_term_counts[["ZIP_CD", "YEAR", "count"]]
```

```{python}
#now I'll test whether the zips listed in when_term_results actually change in the year after shown
term_p2016["year+"] = term_p2016["YEAR"] + 1

#eliminate any duplicates 
zips_term_counts = zips_term_counts.groupby(["ZIP_CD", "YEAR"], as_index=False).agg({"count": "sum"})

#merge on year terminated
results1 = zips_term_counts.merge(term_p2016[["ZIP_CD", "YEAR"]],
  how = "right")
results1 = results1[["ZIP_CD", "YEAR", "count"]]

#merge on next year
results2 = zips_term_counts.merge(term_p2016[["ZIP_CD", "year+"]],
  left_on = ["ZIP_CD", "YEAR"],
  right_on = ["ZIP_CD", "year+"],
  how = "right")
results2 = results2[["ZIP_CD", "YEAR", "count"]]

#merge together to find where counts overlap
results = results1.merge(results2,
  on = ["ZIP_CD"])
results.head(10)
len(results)
```

```{python}
#pulling out the non-terminated bunch
nterminated = results[results["count_x"] == results["count_y"]]
nterminated.head()

print("Based on the above,",len(nterminated),"of", len(term_p2016), "zip codes are not true terminations")
```

    b.
```{python}
#do reverse of n terminated and keep only those zips

terminated = results[results["count_x"] != results["count_y"]]
terminated_df = pos_df[pos_df["ZIP_CD"].isin(terminated["ZIP_CD"])]
terminated_df = terminated_df.groupby("FAC_NAME").size().reset_index()
terminated_df = terminated_df["FAC_NAME"]

print("After first pass,", len(terminated_df), "hospitals remain")
```

```{python}
#finding the non-mergers
term_non_mergers = pos_df[pos_df["ZIP_CD"].isin(terminated["ZIP_CD"])]
term_non_mergers= term_non_mergers[term_non_mergers["PGM_TRMNTN_CD"] != 1]
term_non_mergers = term_non_mergers.groupby("FAC_NAME").size().reset_index()
term_non_mergers = term_non_mergers["FAC_NAME"]

print("After removing voluntary mergers,", len(term_non_mergers), "hospitals remain")
```
    c.
```{python}
#first 10 remaining hospitals
print(term_non_mergers.sort_values().head(10))
```

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are: 
    .dbf: contains information about the file, the data records, and the end of the file
    .prj: contains information about the coordinate system so the data can be projected onto a map 
    .shp: contains information about geographic information and attributes of the data 
    .shx: contains information about the shape files that are used for fonts and line types 
    .xml: contains data that is in plain text format

    b. The size of the files are as follows. These are listed in order from biggest to smallest: 
    .shp: 837,544,580 bytes (852.9 MB on disk)
    .dbf: 6,425,474 bytes (6.4 MB on disk)
    .shx: 265,060 bytes (266 KB on disk)
    .xml: 15,639 bytes (16 KB on disk)
    .prj: 165 bytes (4 KB on disk)
2. 

```{python}
#load in necessary package
import shapely
import geopandas as gpd
```

```{python}
#load in the data
shp_path = "/Users/hallielovin/Documents/GitHub/problem-set-4-ben_hallie/gz_2010_us_860_00_500k"

ben_shp_path = f"/Users/benschiffman/Desktop/Python 2/problem-set-4-ben_hallie/gz_2010_us_860_00_500k"


shp = gpd.read_file(shp_path)
```

```{python}
#Pull out only the Texas zip codes
##indicate what the TX zip codes are 
texas_code = ("75", "76", "77", "78", "79")

#make a new dataframe that only pulls out those rows that are from texas 
texas = shp[shp["ZCTA5"].astype(str).str.startswith(texas_code)]
```

```{python}
#Calculate the number of hospitals per zipcode based on the pos2016_df from part 1 

##restrict this to tx 
texas_2016 = pos2016_df[pos2016_df["ZIP_CD"].astype(str).str.startswith(texas_code)]

#group by zipcode and count the number of hospitals
zip_count = texas_2016.groupby("ZIP_CD").agg(
  NUM_HOSPITALS = ("ZIP_CD", "size")
).reset_index()
```

```{python}
#Clean up the zip codes a bit -- needed chatgpt help on this since it was not working at first
zip_count["ZIP_CD"] = zip_count["ZIP_CD"].astype(str).str.replace('.0', '', regex=False).str.strip()

shp["ZCTA5"] = shp["ZCTA5"].astype(str).str.strip()

#merge the datasets by zip code
texas = texas.merge(zip_count, left_on="ZCTA5", right_on="ZIP_CD", how="left")

#Make anything that is NA = to 0
texas["NUM_HOSPITALS"] = texas["NUM_HOSPITALS"].fillna(0)
```


```{python}
import matplotlib.pyplot as plt
```

```{python}
texas.plot(column = "NUM_HOSPITALS", legend = True)
```

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
```{python}
zips_all_centroids = shp.copy()
zips_all_centroids = zips_all_centroids.to_crs(epsg=3857) #this is from chatGPT after I recieved a message about this issue
zips_all_centroids["centroid"] = zips_all_centroids.centroid
zips_all_centroids.head()
zips_all_centroids.area
```
Source for the below answers:
https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2024/TGRSHP2024_TechDoc.pdf

Column1: GEO_ID refers to a unique alpha numeric identifier used by the census bureau for geographic areas for which is tabulates data.
Column2: ZCTA5 is the 5 digit zip code of the tabulation area.
Column 3: Name in this case just repeats zip code, but in other cases may refer to the actual name of an area.
Column 4: LSAD = Legal statistical area description
Col5: CENSUSAREA
Geometry: Eitehr municipal or otherwise designated areas for statistical purposes
https://www.census.gov/programs-surveys/geography/about/glossary.html
Column6: GEOMETRY contains the WKT pologyon data for each census area.
Column7: Centroid contains the points that represent the centroids of each census area.

2. 
```{python}
zips_texas_centroids = zips_all_centroids[zips_all_centroids["ZCTA5"].astype(str).str.startswith(texas_code)]
len(zips_texas_centroids)
texas_border_codes = ("87", "880", "881", "882", "883", "884", "73", "74", "70", "71", "72")
all_codes = texas_code + texas_border_codes
zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids["ZCTA5"].astype(str).str.startswith(all_codes)]

print("zips_texas_centroids zips:", len(zips_texas_centroids), "\nzzips_texas_borderstates_centroids:", len(zips_texas_borderstates_centroids))
```
3. 
```{python}
#Create zip code list from 2016 of >= 1active hospitals
zips_2016 = active_2016.copy()
zips_2016 = zips_2016.groupby("ZIP_CD").size().reset_index()

#manipulate so merge will work
zips_2016["ZIP_CD"] = zips_2016["ZIP_CD"].str.strip()

#merge
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
  zips_2016,
  left_on = "ZCTA5",
  right_on = "ZIP_CD",
  how = "inner"
)
len(zips_withhospital_centroids)
zips_withhospital_centroids.tail()
```

I used an inner merge to preserve only rows where ZIP_CD and ZCTA5 matched. These are the variables I merged on, both represent zip code. In the output there are some redundant columns as a result, can be dealt with later.
4. 
    a.
```{python}
import time
start = time.time()
ten_zips = zips_texas_centroids.head(10)
ten_zip_test = gpd.sjoin_nearest(
  ten_zips,
  zips_withhospital_centroids,
  how = "inner",
  distance_col = "distance"
)
end = time.time()

time_test = end - start
print(time_test)
```
Given that this took .35 seconds and there are 1935 zips in zips_texas_centoids, I expect the entire operation to take about 68 seconds.
    b.
```{python}
start2 = time.time()
zips_texas_centroids_distances = gpd.sjoin_nearest(
  zips_texas_centroids,
  zips_withhospital_centroids,
  how = "inner",
  distance_col = "distance"
)
end2 = time.time()

time_test2 = end2 - start2
print(time_test2)
```
I was wrong! It took 94.71 seconds. This means that the time growth is not linear.
    c.
    
    so interestingly enough, the units are in degrees. That said, I already converted to meters because an earlier warning message, and some help from the chatGPT, reccomended I did so. The following answer will convert from meters to miles.
```{python}
zips_texas_centroids_distances["distance"] = zips_texas_centroids_distances["distance"] * 0.000621371
```
5. 
    a.
```{python}
zips_texas_centroids_distances
mean_dist_to_hospital_tx = zips_texas_centroids_distances.groupby("ZIP_CD").agg(
  mean_distance = ("distance", "mean"),
  geometry = ("geometry", "first")
  ).reset_index()
```
The distance is in miles
    b.
```{python}
mean_dist = mean_dist_to_hospital_tx["mean_distance"].mean()
mean_dist
```
The mean as indicated above is 1.73 miles. This distance doesn't really make sense because the question iself is somewhat confusing, what does it mean to be the closest hospital to a zip code? What the merge we did only finds the closest zip code with a hospital... so this may not be all that accurate. 
    c.
```{python}
mean_dist_to_hospital_tx = gpd.GeoDataFrame(mean_dist_to_hospital_tx, geometry='geometry')
mean_dist_to_hospital_tx.plot(column = "mean_distance", legend = True)
```
    
## Effects of closures on access in Texas (15 pts)

1. 

```{python}
#create a df of the facilities that had a closure in Texas based on the term_p2016 chart made in previous steps
texas_closure = term_p2016[term_p2016["ZIP_CD"].astype(str).str.startswith(texas_code)]
```

```{python}
#group my zip code and count the number of closures by each zipcode 
tx_zip_closures = texas_closure.groupby("ZIP_CD").size().reset_index(name = "NUM_CLOSURES")

tx_zip_closures["ZIP_CD"].astype(str)
print(tx_zip_closures)
```

2. 

```{python}
#convert zip codes so string and strip them
tx_zip_closures["ZIP_CD"] = tx_zip_closures["ZIP_CD"].astype(str).str.strip()

texas["ZCTA5"] = texas["ZCTA5"].astype(str).str.strip()

#combine the zip codes with closures to the original texas shape file and only keep those that are in the shape file
shp_texas_closure = texas.merge(tx_zip_closures, left_on="ZCTA5", right_on="ZIP_CD", how="left")

shp_texas_closure["NUM_CLOSURES"] = shp_texas_closure["NUM_CLOSURES"].fillna(0)

shp_texas_closure.head()
```

```{python}
#make a chloropleth to indicate where the zip codes are that had at least one closure 
shp_texas_closure.plot(column = "NUM_CLOSURES", legend = True)
```

There were 33 affected zip codes that had at least one closure and are directly affected. 

3. 

```{python}
#create the buffer distance to convert 10 miles into meters 
buffer_distance = 10 * 1609.34
```

```{python}
#had to use chatgpt to help me fix this because without this line of code it was generating one large circle
shp_texas_closure = shp_texas_closure.to_crs(epsg=32614)

#filter shp_texas_closure to only show those that had closures
yes_closed = shp_texas_closure[shp_texas_closure["NUM_CLOSURES"] > 0]

#make a chloropleth that includes the buffer 
yes_closed.buffer(buffer_distance).plot(edgecolor="white")
plt.axis("off")
```

```{python}
#make a new df for zip codes indirectly affected 
shp_buffered_zip = yes_closed.copy()

#add a new column for geometry that is for the buffered distance
shp_buffered_zip["geometry"] = shp_buffered_zip.geometry.buffer(buffer_distance)
```

```{python}
len(shp_buffered_zip)
```

```{python}
#make sure they are the same crs
if texas.crs != shp_buffered_zip.crs:
    shp_buffered_zip = shp_buffered_zip.to_crs(texas.crs)

#do the spatial join
indirect_zips = gpd.sjoin(texas, shp_buffered_zip, how="inner",predicate = "intersects")
```

```{python}
#find how many zip codes are indirectly affected 
indirect_zips["ZCTA5_left"].nunique()
```

There are 609 zip codes that are indirectly affected by hospital closures. 

4. 

```{python}
#make a new column in the texas shapefile which indicates if a zip code is affected by a closure 
texas["AFFECT"] = "Not Affected"
```

```{python}
# pull out the zip codes that were indirectly affected
indirectly_affected = indirect_zips["ZIP_CD_x"].unique()

texas.loc[texas["ZCTA5"].isin(indirectly_affected), "AFFECT"] = "Indirectly Affected"
```
```{python}
#pull out the zip codes that are directly affected
directly_affected = yes_closed["ZCTA5"].unique()

#find these in the texas file and label them as Directly Affected
texas.loc[texas["ZCTA5"].isin(directly_affected), "AFFECT"] = "Directly Affected"
```

```{python}
#make a chloropleth and color code based on the AFFECT category 
texas.plot(column = "AFFECT", legend = True)
plt.axis("off")
```

## Reflecting on the exercise (10 pts) 
1. 
Using the first pass method could cause some issues, one of which being, when we use this method we are not seeing the exact year that a hosptial closes, we are just assuming. That said, a hospital could have actually closed in 2016, but because of the way it is coded in the data we are forced to just assume 2017. Additionally, if hospitals do not appear in the data it is assumed that they are no longer active. This is a big assumption. A hospital may not appear in data for numerous other reasons, such as a data collection misshap or it may have change categories of the type of hospital it is. It is unfair to assume that the hospital is no longer a funcitoning facility. 

We can do a better job of confirming hopsital closures by having hospitals submit forms when they are officially closed to data collection agencies so they are not forced to assume. 