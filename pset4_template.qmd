---
title: "Problem Set 4"
author: "Ben Schiffman and Hallie Lovin"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

```{python}
#import necessary packages
import pandas as pd 
import altair as alt
```

1. 
The variables that I pulled were: PRVDR_CTGRY_SBTYP_CD - shows the subtype of the provider
PRVDR_CTGRY_CD - shows the type of provider 
CITY_NAME - shows the city that the provider is physically located in 
FAC_NAME - shows the name of the provider that is providing services 
PRVDR_NUM - shows the CMS certification number 
STATE_CD - shows the state abbreviation
ST_ADR - shows the street address where the provider is located 
PGM_TRMNTN_CD - shows the termination status of the provider 
TRMNTN_EXPRTN_DT - shows the date that the provider was terminated 
ZIP_CD - shows the zip code of the providers physical address 
FIPS_STATE_CD - shows the FIPS state code 
CBSA_URBN_RRL_IND - shows if the town is urban or rural
2. 

```{python}
#import the data 
import os

path = r"/Users/hallielovin/Documents/GitHub/problem-set-4-ben_hallie"
ben_path = r"/Users/benschiffman/Desktop/Python 2/problem-set-4-ben_hallie"
pos2016 = r"pos2016.csv"

pos2016_df = pd.read_csv(os.path.join(path, pos2016))

#convert zip code to a string 
pos2016_df["ZIP_CD"] = pos2016_df["ZIP_CD"].astype(str).str.replace(".0", " ", regex = False)

pos2016_df.head()
```

```{python}
#subset data to only include those with provider type code 1 and subtype code 1
pos2016_df = pos2016_df[(pos2016_df["PRVDR_CTGRY_CD"] == 1) & (pos2016_df["PRVDR_CTGRY_SBTYP_CD"] == 1)]
```

```{python}
#determine how many hospitals are reported in the data 
len(pos2016_df)
```

    a. There are 7,245 hospitals reported in the data that are considered "short term" hospitals. According to the brief by the Kaiser Family Foundation, there are around 5,000 short term, acute care hospitals in the US. That said, the numbers in this data seem much higher. 

    b. After looking at more sources, a CMS report showed that there were 3,436 medicaid participating hospitals in 2016. The reasons that all of these numbers may differ is because some hospitals may have closed which we have not yet accounted for in our data. Additionally, the Kaiser Foundation is looking at all hospitals that are short term and not filtering out those that bill medicare and medicaid. 
3. 

```{python}
#load in the data from 2017, 2018, and 2019

##2017
pos2017 = r"pos2017.csv"

pos2017_df = pd.read_csv(os.path.join(path, pos2017))

#convert zips to string 
pos2017_df["ZIP_CD"] = pos2017_df["ZIP_CD"].astype(str).str.replace(".0", " ", regex = False)

pos2017_df.head()
```

```{python}
##2018-- I was getting an error (UnicodeDecodeError) so I had to get chatgpt to help 
pos2018 = r"pos2018.csv"

pos2018_df = pd.read_csv(os.path.join(path, pos2018), encoding="ISO-8859-1")

#convert zip to string 
pos2018_df["ZIP_CD"] = pos2018_df["ZIP_CD"].astype(str).str.replace(".0", " ", regex = False)

pos2018_df.head()
```
```{python}
##2019-- I was getting an error (UnicodeDecodeError) so I had to get chatgpt to help 
pos2019 = r"pos2019.csv"

pos2019_df = pd.read_csv(os.path.join(path, pos2019), encoding="ISO-8859-1")

#convert zip to string 
pos2019_df["ZIP_CD"] = pos2019_df["ZIP_CD"].astype(str).str.replace(".0", " ", regex = False)

pos2019_df.head()
```

```{python}
#subset out the dataframes so only the code 1's are shown 
##2017 
pos2017_df = pos2017_df[(pos2017_df["PRVDR_CTGRY_CD"] == 1) & (pos2017_df["PRVDR_CTGRY_SBTYP_CD"] == 1)]
```

```{python}
##2018
pos2018_df = pos2018_df[(pos2018_df["PRVDR_CTGRY_CD"] == 1) & (pos2018_df["PRVDR_CTGRY_SBTYP_CD"] == 1)]
```

```{python}
##2019
pos2019_df = pos2019_df[(pos2019_df["PRVDR_CTGRY_CD"] == 1) & (pos2019_df["PRVDR_CTGRY_SBTYP_CD"] == 1)]
```
```{python}
#append all of the data sets together into one pos data set using the CMS certification number (prvdr_num)

##add a year column to each dataset 
pos2016_df["YEAR"] = 2016
pos2017_df["YEAR"] = 2017
pos2018_df["YEAR"] = 2018
pos2019_df["YEAR"] = 2019
```

```{python}
##make the large dataframe
pos_df = pd.concat([pos2016_df, pos2017_df, pos2018_df, pos2019_df], ignore_index = True)
```

```{python}
#group the dataset by year and take the count of how many observations are in each year 
pos_years_count = pos_df.groupby("YEAR").agg(
  NUM_OBSERVATIONS = ("YEAR", "size")
).reset_index()
```

```{python}
#make a plot of the number of observations by year 
alt.Chart(pos_years_count).mark_bar().encode(
  alt.X("YEAR:O"), 
  alt.Y("NUM_OBSERVATIONS:Q")
)
```

4. 

```{python}
#make a new dataframe that groups the data by the year counting the unique provider numbers
pos_unique_count = pos_df.groupby("YEAR")["PRVDR_NUM"].nunique().reset_index()
```

```{python}
#make a chart 
alt.Chart(pos_unique_count).mark_bar().encode(
  alt.X("YEAR:O"), 
  alt.Y("PRVDR_NUM:Q")
)
```

    a. These plots look the exact same and that is because the numbers are the same that it is taking in.
    b. This tells us that the data is only contains one observation for each hospital in each year. There are no duplicates for the hospitals.

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
#making 2016 list of actives
active_2016 = pos_df[pos_df["PGM_TRMNTN_CD"] == 0]
active_2016 = active_2016[pos_df["YEAR"] == 2016]

#sort by 2016 actives
term_p2016 = pos_df[(pos_df["FAC_NAME"].isin(active_2016["FAC_NAME"])) & (pos_df["YEAR"]> 2016)]

#finding rows where termination code isnt 0
term_p2016 = term_p2016[term_p2016["PGM_TRMNTN_CD"] != 0]

#group by facility name then only retain the lowest year
min_year = term_p2016.groupby("FAC_NAME")["YEAR"].min().reset_index()

#merging back to include more info, in this case just zip code
when_term_results = min_year.merge(term_p2016[["FAC_NAME", "YEAR", "ZIP_CD"]],
  on = ["FAC_NAME", "YEAR"],
  how = "left"
  )
print(len(when_term_results), "hospitals fit this definition")
#consult with hallie on repeats by different zips
```

378 hospitals fit this definition

2. 
```{python}
#sort values by facility name
when_term_results = when_term_results.sort_values(by = "FAC_NAME")

#printing top 10 results
print(when_term_results.head(10))
```

3. 
    a.
```{python}
#first create a df that will count active hospitals by zip by year
active_zips = pos_df.copy()
active_zips["terminated"] = active_zips["PGM_TRMNTN_CD"].apply(lambda x: "no" if x == 0 else "yes")
active_zips["ZIP_CD"]

zips_term_counts = active_zips.groupby(["ZIP_CD", "terminated", "YEAR"]).size().reset_index(name = "count")

#test that there are actually unique results in here
zips_term_counts
zips_term_counts["count"].unique()
zips_term_counts = zips_term_counts[["ZIP_CD", "YEAR", "count"]]
```

```{python}
#now I'll test whether the zips listed in when_term_results actually change in the year after shown
when_term_results["year+"] = when_term_results["YEAR"] + 1

#eliminate any duplicates 
zips_term_counts = zips_term_counts.groupby(["ZIP_CD", "YEAR"], as_index=False).agg({"count": "sum"})

#merge on year terminated
results1 = zips_term_counts.merge(when_term_results[["ZIP_CD", "YEAR"]],
  how = "right")
results1 = results1[["ZIP_CD", "YEAR", "count"]]

#merge on next year
results2 = zips_term_counts.merge(when_term_results[["ZIP_CD", "year+"]],
  left_on = ["ZIP_CD", "YEAR"],
  right_on = ["ZIP_CD", "year+"],
  how = "right")
results2 = results2[["ZIP_CD", "YEAR", "count"]]

#merge together to find where counts overlap
results = results1.merge(results2,
  on = ["ZIP_CD"])
results.head(10)
len(results)
```

```{python}
#pulling out the non-terminated bunch
nterminated = results[results["count_x"] == results["count_y"]]

print("Based on the above,",len(nterminated),"of", len(when_term_results), "zip codes are not true terminations")
```

    b.
```{python}
#do reverse of n terminated and keep only those zips

terminated = results[results["count_x"] != results["count_y"]]
terminated_df = pos_df[pos_df["ZIP_CD"].isin(terminated["ZIP_CD"])]
terminated_df = terminated_df.groupby("FAC_NAME").size().reset_index()
terminated_df = terminated_df["FAC_NAME"]

print("After first pass,", len(terminated_df), "hospitals remain")
```

```{python}
#finding the non-mergers
term_non_mergers = pos_df[pos_df["ZIP_CD"].isin(terminated["ZIP_CD"])]
term_non_mergers= term_non_mergers[term_non_mergers["PGM_TRMNTN_CD"] != 1]
term_non_mergers = term_non_mergers.groupby("FAC_NAME").size().reset_index()
term_non_mergers = term_non_mergers["FAC_NAME"]

print("After removing voluntary mergers,", len(term_non_mergers), "hospitals remain")
```
    c.
```{python}
#first 10 remaining hospitals
print(term_non_mergers.sort_values().head(10))
```

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are: 
    .dbf: contains information about the file, the data records, and the end of the file
    .prj: contains information about the coordinate system so the data can be projected onto a map 
    .shp: contains information about geographic information and attributes of the data 
    .shx: contains information about the shape files that are used for fonts and line types 
    .xml: contains data that is in plain text format

    b. The size of the files are as follows. These are listed in order from biggest to smallest: 
    .shp: 837,544,580 bytes (852.9 MB on disk)
    .dbf: 6,425,474 bytes (6.4 MB on disk)
    .shx: 265,060 bytes (266 KB on disk)
    .xml: 15,639 bytes (16 KB on disk)
    .prj: 165 bytes (4 KB on disk)
2. 

```{python}
#load in necessary package
import shapely
import geopandas as gpd
```

```{python}
#load in the data
shp_path = "/Users/hallielovin/Documents/GitHub/problem-set-4-ben_hallie/gz_2010_us_860_00_500k"

shp = gpd.read_file(shp_path)
```

```{python}
#Pull out only the Texas zip codes
##indicate what the TX zip codes are 
texas_code = ("75", "76", "77", "78", "79")

#make a new dataframe that only pulls out those rows that are from texas 
texas = shp[shp["ZCTA5"].astype(str).str.startswith(texas_code)]
```

```{python}
#Calculate the number of hospitals per zipcode based on the pos2016_df from part 1 

##restrict this to tx 
texas_2016 = pos2016_df[pos2016_df["ZIP_CD"].astype(str).str.startswith(texas_code)]

#group by zipcode and count the number of hospitals
zip_count = texas_2016.groupby("ZIP_CD").agg(
  NUM_HOSPITALS = ("ZIP_CD", "size")
).reset_index()
```

```{python}
#Clean up the zip codes a bit -- needed chatgpt help on this since it was not working at first
zip_count["ZIP_CD"] = zip_count["ZIP_CD"].astype(str).str.replace('.0', '', regex=False).str.strip()

shp["ZCTA5"] = shp["ZCTA5"].astype(str).str.strip()

#merge the datasets by zip code
texas = texas.merge(zip_count, left_on="ZCTA5", right_on="ZIP_CD", how="left")

#Make anything that is NA = to 0
texas["NUM_HOSPITALS"] = texas["NUM_HOSPITALS"].fillna(0)
```


```{python}
import matplotlib.pyplot as plt
```

```{python}
texas.plot(column = "NUM_HOSPITALS", legend = True)
```



```{python}
pos2016_df[pos2016_df["ZIP_CD"] == "751"]
```

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
